{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/isaac/miniconda3/envs/3dvision/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "BASE_DIR = os.path.join(BASE_DIR, \"pointnet_semseg_vkitti\" )\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'data'))\n",
    "import provider\n",
    "import tf_util\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "BATCH_SIZE_EVAL = 24\n",
    "NUM_POINT = 4096\n",
    "MAX_EPOCH = 50\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 300000\n",
    "DECAY_RATE = 0.5\n",
    "\n",
    "LOG_DIR = 'log'\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp model.py %s' % (LOG_DIR)) # bkp of model def\n",
    "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "MAX_NUM_POINT = 4096\n",
    "NUM_CLASSES = 13\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "#BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob('./data/vkitti3d_dataset_v1.0/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FILES = []\n",
    "for folder in folders:\n",
    "    np_arrays = glob.glob(folder + \"/*\")\n",
    "    ALL_FILES += np_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/vkitti3d_dataset_v1.0/01/0001_00000.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00012.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00024.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00036.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00048.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00060.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00072.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00085.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00097.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00109.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00121.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00133.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00145.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00157.npy',\n",
       " './data/vkitti3d_dataset_v1.0/01/0001_00170.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00230.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00243.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00257.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00270.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00284.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00297.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00311.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00325.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00338.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00352.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00365.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00379.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00392.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00406.npy',\n",
       " './data/vkitti3d_dataset_v1.0/02/0001_00420.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00000.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00015.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00031.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00047.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00063.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00079.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00095.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00111.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00127.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00143.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00159.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00175.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00191.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00207.npy',\n",
       " './data/vkitti3d_dataset_v1.0/03/0002_00223.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00030.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00052.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00074.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00096.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00118.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00140.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00162.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00184.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00206.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00228.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00250.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00272.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00294.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00316.npy',\n",
       " './data/vkitti3d_dataset_v1.0/04/0018_00338.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00080.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00106.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00132.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00158.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00184.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00210.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00236.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00262.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00288.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00314.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00340.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00366.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00392.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00418.npy',\n",
       " './data/vkitti3d_dataset_v1.0/05/0020_00444.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00500.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00521.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00542.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00564.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00585.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00607.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00628.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00650.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00671.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00692.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00714.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00735.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00757.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00778.npy',\n",
       " './data/vkitti3d_dataset_v1.0/06/0020_00800.npy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ALL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4096, 6)\n",
      "(90, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Load ALL data\n",
    "total_data = np.empty((90, 4096, 6))\n",
    "total_label = np.empty((90, 4096))\n",
    "\n",
    "for i, file in enumerate(ALL_FILES):\n",
    "    f = np.load(file)  # shape: (401326, 7)\n",
    "    data = f[:,:6]\n",
    "    data = np.reshape(data, (1, data.shape[0], data.shape[1]))\n",
    "    label = f[:,-1]\n",
    "    label = np.reshape(label, (1, label.shape[0]))\n",
    "    \n",
    "    idxs = np.arange(0, data.shape[1])\n",
    "    np.random.shuffle(idxs)\n",
    "    total_data[i,:,:] = data[:, idxs[:4096], :]\n",
    "    total_label[i,:] = label[:, idxs[:4096]]\n",
    "    \n",
    "print(total_data.shape)\n",
    "print(total_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_range : 2.1171770095825195 149.8768768310547\n",
      "y_range : -96.41452026367188 95.5736312866211\n",
      "z_range : -3.9555277824401855 30.010095596313477\n",
      "r_range : 0.0 255.0\n",
      "g_range : 0.0 255.0\n",
      "b_range : 0.0 255.0\n"
     ]
    }
   ],
   "source": [
    "features = [\"x\",\"y\",\"z\",\"r\",\"g\",\"b\"]\n",
    "for i in range(6): \n",
    "    print(features[i] + \"_range :\", np.min(total_data[:, :, i]), np.max(total_data[:, :, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 4096, 6), (90, 4096))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = total_data\n",
    "y = total_label\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1171770095825195, -96.41452026367188, -3.9555277824401855, 0.0, 0.0, 0.0]\n",
      "[149.8768768310547, 95.5736312866211, 30.010095596313477, 255.0, 255.0, 255.0]\n"
     ]
    }
   ],
   "source": [
    "xmin = []\n",
    "xmax = []\n",
    "for i in range(6):\n",
    "    xmin.append(np.min(X[:,:,i]))\n",
    "    xmax.append(np.max(X[:,:,i]))\n",
    "    \n",
    "print(xmin)\n",
    "print(xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normal = np.zeros(X.shape)\n",
    "for i in range(6):\n",
    "    X_normal[:,:,i] = (X[:,:,i] - xmin[i]) / (xmax[i] - xmin[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_range : 0.0 1.0\n",
      "y_range : 0.0 1.0\n",
      "z_range : 0.0 1.0\n",
      "r_range : 0.0 1.0\n",
      "g_range : 0.0 1.0\n",
      "b_range : 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "features = [\"x\",\"y\",\"z\",\"r\",\"g\",\"b\"]\n",
    "for i in range(6): \n",
    "    print(features[i] + \"_range :\", np.min(X_normal[:, :, i]), np.max(X_normal[:, :, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 4096, 6) (66, 4096)\n",
      "(24, 4096, 6) (24, 4096)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(X_normal, y, test_size=0.26, random_state=42)\n",
    "\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc2/Relu:0\", shape=(24, 128), dtype=float32, device=/device:GPU:0)\n",
      "**** EPOCH 000 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 2.537423\n",
      "accuracy: 0.254990\n",
      "----\n",
      "eval mean loss: 43.578381\n",
      "eval accuracy: 0.268188\n",
      "eval avg class acc: 0.150831\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 001 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 1.166659\n",
      "accuracy: 0.697266\n",
      "----\n",
      "eval mean loss: 4.697268\n",
      "eval accuracy: 0.423299\n",
      "eval avg class acc: 0.293540\n",
      "**** EPOCH 002 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.867883\n",
      "accuracy: 0.757619\n",
      "----\n",
      "eval mean loss: 1.565815\n",
      "eval accuracy: 0.690664\n",
      "eval avg class acc: 0.366182\n",
      "**** EPOCH 003 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.702951\n",
      "accuracy: 0.799886\n",
      "----\n",
      "eval mean loss: 1.117834\n",
      "eval accuracy: 0.748820\n",
      "eval avg class acc: 0.385326\n",
      "**** EPOCH 004 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.613740\n",
      "accuracy: 0.826279\n",
      "----\n",
      "eval mean loss: 0.902869\n",
      "eval accuracy: 0.788208\n",
      "eval avg class acc: 0.414388\n",
      "**** EPOCH 005 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.536839\n",
      "accuracy: 0.842356\n",
      "----\n",
      "eval mean loss: 0.757961\n",
      "eval accuracy: 0.804413\n",
      "eval avg class acc: 0.436414\n",
      "**** EPOCH 006 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.465048\n",
      "accuracy: 0.863663\n",
      "----\n",
      "eval mean loss: 0.644301\n",
      "eval accuracy: 0.820068\n",
      "eval avg class acc: 0.450874\n",
      "**** EPOCH 007 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.457348\n",
      "accuracy: 0.862757\n",
      "----\n",
      "eval mean loss: 0.582490\n",
      "eval accuracy: 0.831095\n",
      "eval avg class acc: 0.465062\n",
      "**** EPOCH 008 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.451938\n",
      "accuracy: 0.860596\n",
      "----\n",
      "eval mean loss: 0.536842\n",
      "eval accuracy: 0.844187\n",
      "eval avg class acc: 0.483288\n",
      "**** EPOCH 009 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.373102\n",
      "accuracy: 0.887629\n",
      "----\n",
      "eval mean loss: 0.537272\n",
      "eval accuracy: 0.841227\n",
      "eval avg class acc: 0.484336\n",
      "**** EPOCH 010 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.352074\n",
      "accuracy: 0.894333\n",
      "----\n",
      "eval mean loss: 0.517069\n",
      "eval accuracy: 0.842967\n",
      "eval avg class acc: 0.482624\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 011 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.358373\n",
      "accuracy: 0.891978\n",
      "----\n",
      "eval mean loss: 0.482468\n",
      "eval accuracy: 0.856242\n",
      "eval avg class acc: 0.500174\n",
      "**** EPOCH 012 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.314100\n",
      "accuracy: 0.905741\n",
      "----\n",
      "eval mean loss: 0.462273\n",
      "eval accuracy: 0.858663\n",
      "eval avg class acc: 0.519878\n",
      "**** EPOCH 013 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.321878\n",
      "accuracy: 0.903000\n",
      "----\n",
      "eval mean loss: 0.457503\n",
      "eval accuracy: 0.861084\n",
      "eval avg class acc: 0.521809\n",
      "**** EPOCH 014 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.284264\n",
      "accuracy: 0.914510\n",
      "----\n",
      "eval mean loss: 0.469695\n",
      "eval accuracy: 0.864583\n",
      "eval avg class acc: 0.504424\n",
      "**** EPOCH 015 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.279930\n",
      "accuracy: 0.914948\n",
      "----\n",
      "eval mean loss: 0.448061\n",
      "eval accuracy: 0.866475\n",
      "eval avg class acc: 0.511495\n",
      "**** EPOCH 016 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.270441\n",
      "accuracy: 0.919027\n",
      "----\n",
      "eval mean loss: 0.418318\n",
      "eval accuracy: 0.872406\n",
      "eval avg class acc: 0.538813\n",
      "**** EPOCH 017 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.259530\n",
      "accuracy: 0.922536\n",
      "----\n",
      "eval mean loss: 0.416066\n",
      "eval accuracy: 0.876607\n",
      "eval avg class acc: 0.547549\n",
      "**** EPOCH 018 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.240547\n",
      "accuracy: 0.929835\n",
      "----\n",
      "eval mean loss: 0.426296\n",
      "eval accuracy: 0.870697\n",
      "eval avg class acc: 0.550107\n",
      "**** EPOCH 019 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.240687\n",
      "accuracy: 0.929011\n",
      "----\n",
      "eval mean loss: 0.427838\n",
      "eval accuracy: 0.878601\n",
      "eval avg class acc: 0.539055\n",
      "**** EPOCH 020 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.221030\n",
      "accuracy: 0.933314\n",
      "----\n",
      "eval mean loss: 0.409941\n",
      "eval accuracy: 0.881114\n",
      "eval avg class acc: 0.554812\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 021 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.210678\n",
      "accuracy: 0.935170\n",
      "----\n",
      "eval mean loss: 0.405506\n",
      "eval accuracy: 0.881602\n",
      "eval avg class acc: 0.571796\n",
      "**** EPOCH 022 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.216569\n",
      "accuracy: 0.934550\n",
      "----\n",
      "eval mean loss: 0.415557\n",
      "eval accuracy: 0.884349\n",
      "eval avg class acc: 0.567729\n",
      "**** EPOCH 023 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.206821\n",
      "accuracy: 0.936915\n",
      "----\n",
      "eval mean loss: 0.415547\n",
      "eval accuracy: 0.883311\n",
      "eval avg class acc: 0.568543\n",
      "**** EPOCH 024 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.209082\n",
      "accuracy: 0.936849\n",
      "----\n",
      "eval mean loss: 0.410795\n",
      "eval accuracy: 0.884308\n",
      "eval avg class acc: 0.581911\n",
      "**** EPOCH 025 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.195926\n",
      "accuracy: 0.941803\n",
      "----\n",
      "eval mean loss: 0.402540\n",
      "eval accuracy: 0.886373\n",
      "eval avg class acc: 0.572072\n",
      "**** EPOCH 026 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.195422\n",
      "accuracy: 0.941325\n",
      "----\n",
      "eval mean loss: 0.420266\n",
      "eval accuracy: 0.886882\n",
      "eval avg class acc: 0.558213\n",
      "**** EPOCH 027 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.185683\n",
      "accuracy: 0.944448\n",
      "----\n",
      "eval mean loss: 0.430942\n",
      "eval accuracy: 0.884918\n",
      "eval avg class acc: 0.555302\n",
      "**** EPOCH 028 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.221377\n",
      "accuracy: 0.932648\n",
      "----\n",
      "eval mean loss: 0.397563\n",
      "eval accuracy: 0.887929\n",
      "eval avg class acc: 0.587625\n",
      "**** EPOCH 029 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.168333\n",
      "accuracy: 0.949320\n",
      "----\n",
      "eval mean loss: 0.425734\n",
      "eval accuracy: 0.878469\n",
      "eval avg class acc: 0.604704\n",
      "**** EPOCH 030 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.207485\n",
      "accuracy: 0.935125\n",
      "----\n",
      "eval mean loss: 0.438523\n",
      "eval accuracy: 0.883779\n",
      "eval avg class acc: 0.568709\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 031 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.165596\n",
      "accuracy: 0.949829\n",
      "----\n",
      "eval mean loss: 0.450620\n",
      "eval accuracy: 0.879506\n",
      "eval avg class acc: 0.555368\n",
      "**** EPOCH 032 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.169535\n",
      "accuracy: 0.948278\n",
      "----\n",
      "eval mean loss: 0.416527\n",
      "eval accuracy: 0.886810\n",
      "eval avg class acc: 0.591199\n",
      "**** EPOCH 033 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.163504\n",
      "accuracy: 0.949900\n",
      "----\n",
      "eval mean loss: 0.399857\n",
      "eval accuracy: 0.890177\n",
      "eval avg class acc: 0.596870\n",
      "**** EPOCH 034 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.163234\n",
      "accuracy: 0.949966\n",
      "----\n",
      "eval mean loss: 0.402499\n",
      "eval accuracy: 0.891378\n",
      "eval avg class acc: 0.604930\n",
      "**** EPOCH 035 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.178132\n",
      "accuracy: 0.946701\n",
      "----\n",
      "eval mean loss: 0.422635\n",
      "eval accuracy: 0.889181\n",
      "eval avg class acc: 0.604579\n",
      "**** EPOCH 036 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.159243\n",
      "accuracy: 0.952464\n",
      "----\n",
      "eval mean loss: 0.453977\n",
      "eval accuracy: 0.883423\n",
      "eval avg class acc: 0.588877\n",
      "**** EPOCH 037 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.139736\n",
      "accuracy: 0.957840\n",
      "----\n",
      "eval mean loss: 0.440523\n",
      "eval accuracy: 0.881042\n",
      "eval avg class acc: 0.594163\n",
      "**** EPOCH 038 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.143886\n",
      "accuracy: 0.956711\n",
      "----\n",
      "eval mean loss: 0.425514\n",
      "eval accuracy: 0.881877\n",
      "eval avg class acc: 0.586175\n",
      "**** EPOCH 039 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.139591\n",
      "accuracy: 0.957886\n",
      "----\n",
      "eval mean loss: 0.412836\n",
      "eval accuracy: 0.890310\n",
      "eval avg class acc: 0.599102\n",
      "**** EPOCH 040 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.137886\n",
      "accuracy: 0.957413\n",
      "----\n",
      "eval mean loss: 0.397202\n",
      "eval accuracy: 0.892965\n",
      "eval avg class acc: 0.629461\n",
      "Model saved in file: log/model.ckpt\n",
      "**** EPOCH 041 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.140649\n",
      "accuracy: 0.956675\n",
      "----\n",
      "eval mean loss: 0.404730\n",
      "eval accuracy: 0.898580\n",
      "eval avg class acc: 0.630231\n",
      "**** EPOCH 042 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.154067\n",
      "accuracy: 0.951960\n",
      "----\n",
      "eval mean loss: 0.426870\n",
      "eval accuracy: 0.899343\n",
      "eval avg class acc: 0.620685\n",
      "**** EPOCH 043 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.166424\n",
      "accuracy: 0.949376\n",
      "----\n",
      "eval mean loss: 0.403597\n",
      "eval accuracy: 0.899679\n",
      "eval avg class acc: 0.630669\n",
      "**** EPOCH 044 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.142415\n",
      "accuracy: 0.957418\n",
      "----\n",
      "eval mean loss: 0.409051\n",
      "eval accuracy: 0.892466\n",
      "eval avg class acc: 0.636527\n",
      "**** EPOCH 045 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.133516\n",
      "accuracy: 0.958974\n",
      "----\n",
      "eval mean loss: 0.457201\n",
      "eval accuracy: 0.881673\n",
      "eval avg class acc: 0.582170\n",
      "**** EPOCH 046 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.196413\n",
      "accuracy: 0.939713\n",
      "----\n",
      "eval mean loss: 0.405823\n",
      "eval accuracy: 0.894124\n",
      "eval avg class acc: 0.628999\n",
      "**** EPOCH 047 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.141238\n",
      "accuracy: 0.956406\n",
      "----\n",
      "eval mean loss: 0.406846\n",
      "eval accuracy: 0.895772\n",
      "eval avg class acc: 0.625263\n",
      "**** EPOCH 048 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.123774\n",
      "accuracy: 0.963140\n",
      "----\n",
      "eval mean loss: 0.409056\n",
      "eval accuracy: 0.895142\n",
      "eval avg class acc: 0.617283\n",
      "**** EPOCH 049 ****\n",
      "----\n",
      "Current batch/total batch num: 0/2\n",
      "mean loss: 0.133886\n",
      "accuracy: 0.959696\n",
      "----\n",
      "eval mean loss: 0.410082\n",
      "eval accuracy: 0.901306\n",
      "eval avg class acc: 0.633476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            \n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss \n",
    "            pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = get_loss(pred, labels_pl)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(pred, 2), tf.to_int64(labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE*NUM_POINT)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = True\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Add summary writers\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                                  sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init, {is_training_pl:True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            log_string('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "             \n",
    "            train_one_epoch(sess, ops, train_writer)\n",
    "            eval_one_epoch(sess, ops, test_writer)\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    log_string('----')\n",
    "    current_data, current_label, _ = provider.shuffle_data(train_data[:,0:NUM_POINT,:], train_label) \n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                     ops['is_training_pl']: is_training,}\n",
    "        summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "    \n",
    "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        \n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    current_data = test_data[:,0:NUM_POINT,:]\n",
    "    current_label = np.squeeze(test_label)\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE_EVAL\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE_EVAL\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE_EVAL\n",
    "\n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred']],\n",
    "                                      feed_dict=feed_dict)\n",
    "        test_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE_EVAL*NUM_POINT)\n",
    "        loss_sum += (loss_val*BATCH_SIZE_EVAL)\n",
    "        for i in range(start_idx, end_idx):\n",
    "            for j in range(NUM_POINT):\n",
    "                l = int(current_label[i, j])\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i-start_idx, j] == l)\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen/NUM_POINT)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "         \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    LOG_FOUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
