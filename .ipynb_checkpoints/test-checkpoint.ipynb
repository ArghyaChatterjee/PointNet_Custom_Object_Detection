{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/python3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import provider\n",
    "import tf_util\n",
    "from model import *\n",
    "from plyfile import PlyData, PlyElement\n",
    "print(\"success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "BATCH_SIZE_EVAL = 1\n",
    "NUM_POINT = 4096\n",
    "MAX_EPOCH = 50\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 300000\n",
    "DECAY_RATE = 0.5\n",
    "\n",
    "LOG_DIR = 'log'\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp model.py %s' % (LOG_DIR)) # bkp of model def\n",
    "#os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "MAX_NUM_POINT = 4096\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "#BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"test.h5\" (mode r+)>\n",
      "(4096, 6)\n",
      "(1, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atas/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Load ALL data\n",
    "f = h5py.File('data/test.h5')\n",
    "print(f)\n",
    "\n",
    "#Choose a frame to test, (0,60)\n",
    "frame_to_test = 56\n",
    "\n",
    "\n",
    "test_data = np.zeros((4096, 6))\n",
    "test_label = np.ones((1,4096))\n",
    "\n",
    "xmax = 3.0\n",
    "xmin = -3.0\n",
    "\n",
    "data = f['data']\n",
    "label = f['label']\n",
    "test_data[:,0:3] = (data[frame_to_test][:, 0:3]- xmin) / (xmax  - xmin )\n",
    "test_data[:,3:6] = data[frame_to_test][:, 3:6]\n",
    "test_label[:,:] = label[frame_to_test][:]\n",
    "\n",
    "    \n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_range : 0.38554444909095764 0.608220249414444\n",
      "y_range : 0.36860136191050213 0.6201140483220419\n",
      "z_range : 0.5441666692495346 0.699833333492279\n",
      "r_range : 0.0 0.0039061899296939373\n",
      "g_range : 4.613610144588165e-05 0.003921568859368563\n",
      "b_range : 0.0 0.0039061899296939373\n"
     ]
    }
   ],
   "source": [
    "features = [\"x\",\"y\",\"z\",\"r\",\"g\",\"b\"]\n",
    "for i in range(6): \n",
    "    print(features[i] + \"_range :\", np.min(test_data[:, i]), np.max(test_data[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38554444909095764, 0.36860136191050213, 0.5441666692495346, 0.0, 4.613610144588165e-05, 0.0]\n",
      "[0.608220249414444, 0.6201140483220419, 0.699833333492279, 0.0039061899296939373, 0.003921568859368563, 0.0039061899296939373]\n"
     ]
    }
   ],
   "source": [
    "test_data_min = []\n",
    "test_data_max = []\n",
    "for i in range(6):\n",
    "    test_data_min.append(np.min(test_data[:,i]))\n",
    "    test_data_max.append(np.max(test_data[:,i]))\n",
    "    \n",
    "print(test_data_min)\n",
    "print(test_data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_range : 0.38554444909095764 0.608220249414444\n",
      "y_range : 0.36860136191050213 0.6201140483220419\n",
      "z_range : 0.5441666692495346 0.699833333492279\n",
      "r_range : 0.0 0.0039061899296939373\n",
      "g_range : 4.613610144588165e-05 0.003921568859368563\n",
      "b_range : 0.0 0.0039061899296939373\n"
     ]
    }
   ],
   "source": [
    "features = [\"x\",\"y\",\"z\",\"r\",\"g\",\"b\"]\n",
    "for i in range(6): \n",
    "    print(features[i] + \"_range :\", np.min(test_data[:, i]), np.max(test_data[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atas/PointNet_Custom_Object_Detection/model.py:13: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/atas/PointNet_Custom_Object_Detection/tf_util.py:145: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/atas/PointNet_Custom_Object_Detection/tf_util.py:21: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/atas/PointNet_Custom_Object_Detection/tf_util.py:48: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/atas/PointNet_Custom_Object_Detection/tf_util.py:368: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Tensor(\"fc2/Relu:0\", shape=(1, 128), dtype=float32, device=/device:GPU:0)\n",
      "WARNING:tensorflow:From /home/atas/PointNet_Custom_Object_Detection/tf_util.py:573: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/atas/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/home/atas/PointNet-SemSeg-VKITTI3D/log; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a3e9adaa1cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mLOG_FOUT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a3e9adaa1cf6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/atas/PointNet-SemSeg-VKITTI3D/log/model.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Restore variables from disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mlog_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[1;32m   1278\u001b[0m                        compat.as_text(save_path))\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m    370\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m    371\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m--> 372\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 384\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    385\u001b[0m     ]\n\u001b[1;32m    386\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /home/atas/PointNet-SemSeg-VKITTI3D/log; No such file or directory"
     ]
    }
   ],
   "source": [
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def evaluate():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            \n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss \n",
    "            pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = get_loss(pred, labels_pl)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = True\n",
    "        sess = tf.Session(config=config)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        \n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch}\n",
    "        MODEL_PATH = \"log/model.ckpt\"\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, MODEL_PATH)\n",
    "        log_string(\"Model restored.\")\n",
    "        \n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        eval_one_epoch(sess, ops, test_writer)\n",
    "\n",
    "\n",
    "        \n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    #current_data = np.zeros((1,4096, 6))\n",
    "    current_data  = test_data[0:NUM_POINT,:]\n",
    "    current_label = test_label\n",
    "    \n",
    "    current_data = current_data.reshape(1,4096, 6)\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE_EVAL \n",
    "    \n",
    "    fout = open('/home/atas/pred.obj', 'w')\n",
    "    fout_gt = open('/home/atas/gt.obj', 'w')    \n",
    "    \n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE_EVAL\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE_EVAL\n",
    "\n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[:, :],\n",
    "                     ops['labels_pl']: current_label[:],\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred']],\n",
    "                                      feed_dict=feed_dict)\n",
    "        \n",
    "        pred_label = np.argmax(pred_val, 2) # BxN\n",
    "        \n",
    "        test_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE_EVAL*NUM_POINT)\n",
    "        loss_sum += (loss_val*BATCH_SIZE_EVAL)\n",
    "        class_color = [[0,255,0],[0,0,255]]\n",
    "        print(start_idx, end_idx)\n",
    "        \n",
    "        for i in range(start_idx, end_idx):\n",
    "            print(pred_label.shape)\n",
    "            pred = pred_label[i-start_idx, :]\n",
    "            \n",
    "            pts = current_data[i-start_idx, :, :]\n",
    "            l = current_label[i-start_idx,:]\n",
    "            \n",
    "            for j in range(NUM_POINT):\n",
    "                l = int(current_label[i, j])\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i-start_idx, j] == l)\n",
    " \n",
    "                color = class_color[pred_val[i-start_idx, j]]\n",
    "                color_gt = class_color[l]\n",
    "  \n",
    "                fout.write('v %f %f %f %d %d %d\\n' % (pts[j,0], pts[j,1], pts[j,2], color[0], color[1], color[2]))\n",
    "                fout_gt.write('v %f %f %f %d %d %d\\n' % (pts[j,0], pts[j,1], pts[j,2], color_gt[0], color_gt[1], color_gt[2]))\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen/NUM_POINT)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "         \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n",
    "    LOG_FOUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-7a832d0dda5c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7a832d0dda5c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    eval mean loss: 0.297671\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eval mean loss: 0.297671\n",
    "eval accuracy: 0.840576\n",
    "eval avg class acc: 0.707515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
